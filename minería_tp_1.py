# -*- coding: utf-8 -*-
"""Minería TP 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A70j3Fg9Nv9TzRuk81x4sz9nxt1ChffM

## TUIA 4° cuatrimestre

# Minería de Datos

## Trabajo Práctico N°1

## Integrantes: Menescaldi Brisa, Vercesi Patricio

### 1. Importación de librerías y datos
"""

!pip install gap_stat -q
#!pip install plotly -q
#!pip install scikit-learn -q

from google.colab import files
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

from sklearn.preprocessing import StandardScaler
import itertools

from sklearn.decomposition import PCA
from sklearn.manifold import Isomap
from sklearn.manifold import TSNE
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import cdist
from sklearn.metrics import silhouette_score,silhouette_samples
from gap_statistic import OptimalK

"""**Seleccionar el dataset para subirlo:**"""

files.upload()

df = pd.read_csv('Crop_recommendation.csv')

"""### 2. Analizar los atributos del conjunto de datos (distribuciones, valores, outliers, tipos de datos, etc.) y elegir un método de estandarización."""

df

"""#### Exploración de los datos"""

df.info()

"""Como podemos observar no hay valores faltantes."""

df.describe()

plt.figure(figsize=(12, 6))
df.boxplot()
plt.show()

# Para ver de más cerca las columnas con menos rango:
df[["temperature", "ph"]].boxplot()
plt.show()

df.hist(figsize=(10, 8))
plt.show()

# Chequeamos si hay outliers de la variable categórica:
plt.figure(figsize=(15, 2))
ax = sns.countplot(x='label', data=df)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')
plt.xlabel("Labels")
plt.ylabel("Muestras")
plt.title('Cantidad de muestras por tipo de cultivo')
plt.show()

"""Aunque los boxplots determinen ciertos datos como outliers, en los histogramas podemos ver que hay múltiples datos en todos los valores extremos y en realidad no son atípicos."""

corr = df.drop(columns=["label"]).corr()
ax = sns.heatmap(corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200), annot = True, annot_kws = {'size': 6})
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')
plt.title("Correlaciones de los datos numéricos del dataset")
plt.show()

"""Todas las características están bastante poco correlacionadas

#### Estandarización de los datos

Separaremos los atributos numéricos del que tiene labels porque no aplicaremos reducción de dimensionalidad sobre este último, sino que lo usaremos para observar cuan separadas quedan las categorías después de cada método.
"""

df_x = df.drop('label', axis=1)
df_y = df.label
unique_labels = df_y.unique()

"""Todos los atributos que quedan son numéricos y los escalaremos por Z-score"""

x_s = StandardScaler().fit_transform(df_x)
x_s

"""### 3. PCA

#### Calculamos todas las componentes principales
"""

# Ajuste del modelo y transformación de los datos
pca = PCA(n_components = x_s.shape[1])
pca_features = pca.fit_transform(x_s)
# Guardamos las features en un dataframe
pca_df = pd.DataFrame(data=pca_features, columns=[f'PC{i+1}' for i in range(x_s.shape[1])])
pca_df['label'] = df_y
pca_df.head()

"""#### Averiguamos cuanta varianza explican para ver con cuáles nos quedamos"""

pca_rtd = pd.DataFrame({'explained_variance_ratio':pca.explained_variance_ratio_}, index=pca_df.columns[:-1])
pca_rtd

plt.bar(pca_rtd.index, pca_rtd.explained_variance_ratio, alpha=0.5, align='center')
plt.step(pca_rtd.index, np.cumsum(pca_rtd.explained_variance_ratio), where='mid', color='red')
plt.title('PCA de Cultivos')
plt.ylabel('Proporción de variancia explicada')
plt.xlabel('Componente principales')
plt.show()

"""Usando la regla del ~75%-80% de variancia acumulada explicada decidimos quedarnos con 4 componentes​"""

pca_df_f = pca_df.drop(columns=["PC5", "PC6", "PC7"])
pca_df_f.head()

"""#### Graficamos las 2 componentes principales sobre las características de los distintos cultivos"""

fig = px.scatter(pca_df_f, x='PC1', y='PC2', color = pca_df_f["label"],  labels={'color': 'Tipo'})
fig.update_layout(title = 'Cultivos con Dimensiones Reducidas con PCA de 2 componentes')
fig.show()

"""### 4. Isomap

#### Análisis variando hiperparámetros
"""

# Creación de mapa de colores
label_color_mapping = {label: plt.cm.tab20(i) for i, label in enumerate(unique_labels)}
label_colors = [label_color_mapping[label] for label in df_y]

p_n_neighbors = [2, 6, 12, 20]
p_n_components = [2, 3, 6]
# Creación de todas las combinaciones entre los elementos de cada lista
combinations = list(itertools.product(p_n_neighbors, p_n_components))

# Para ignorar futuros warnings
import warnings
warnings.filterwarnings("ignore")
# Si se quieren volver a ver:
# warnings.filterwarnings("default")

# Ploteamos todas las combinaciones para compararlas
n_plots = len(combinations)
n_cols = 3

fig, axes = plt.subplots(nrows=n_plots//n_cols, ncols=n_cols, figsize=(12, 14))
axes = axes.flatten()

for i in range(n_plots):
    # Parámetros de combinación i
    combination = combinations[i]
    n_neighbors=combination[0]
    n_components=combination[1]

    # Ajuste(s) de modelo(s)
    iso = Isomap(n_neighbors=n_neighbors, n_components=n_components)
    transformed_data = iso.fit_transform(x_s)
    # Ploteo
    axes[i].scatter(transformed_data[:, 0], transformed_data[:, 1], c=label_colors)
    axes[i].set_title(f"Neighbors={n_neighbors}, Components={n_components}")
    axes[i].set_xticks([])
    axes[i].set_yticks([])
    axes[i].set_xlabel('Componente 1')
    axes[i].set_ylabel('Componente 2')
plt.tight_layout()
plt.show()

"""Al ver los resultados de los datos reducidos dimensionalmente, algo importante a observar es que por alguna razón, el número de componentes no parece modificar el modelo final.

Y aunque es complicado decidir cuál es el mejor parámetro de cantidad de vecinos debido a la falta de un objetivo claro, podemos buscar una transformación de los datos en donde los diferentes tipos de cultivos esten más separados entre sí.

Teniendo eso en cuenta y aunque en todos los subplots hay bastante solapamiento, en el caso en el que los nodos se unen de a 6 vecinos es donde parece haber menor solapamiento.

Pero si en vez de eso quisiéramos mantener la mayor cantidad de varianza, quizás deberíamos de elegir otro modelo.

#### Mejor resultado graficado en más detalle
"""

# Entrenamiento del modelo
isomap = Isomap(n_neighbors=6, n_components=2)
transformed_data = isomap.fit_transform(x_s)

# Plot
plt.figure(figsize=(10, 10))
plt.scatter(transformed_data[:, 0], transformed_data[:, 1], c=label_colors)
plt.legend()
custom_legend = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=label_color_mapping[label], label=label) for label in unique_labels]
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), handles=custom_legend, title='Tipo')
plt.title('Cultivos con Dimensiones Reducidas con Isomap de 2 componentes y 6 vecinos')
plt.xlabel('Componente 1')
plt.ylabel('Componente 2')
plt.show()

"""### 5. t-SNE

#### Análisis variando hiperparámetros
"""

# Creación de mapa de colores
label_color_mapping = {label: plt.cm.tab20(i) for i, label in enumerate(unique_labels)}
label_colors = [label_color_mapping[label] for label in df_y]

# Creación de iteraciones posibles
p_n_iter = [250, 300, 500]
p_n_components = [2, 3]
p_perplexities = [5, 30, 50]

# Creación de todas las combinaciones entre los elementos de cada lista
combinations = list(itertools.product(p_n_iter, p_n_components, p_perplexities))

# Ploteamos todas las combinaciones para compararlas
n_plots = len(combinations)
n_cols = 3

fig, axes = plt.subplots(nrows=n_plots//n_cols, ncols=n_cols, figsize=(12, 14))
axes = axes.flatten()

for i in range(n_plots):
    # Parámetros de combinación i
    combination = combinations[i]
    n_iter=combination[0]
    n_components=combination[1]
    perplexity=combination[2]

    # Ajuste(s) de modelo(s) y transformación de los datos
    tsne = TSNE(n_iter=n_iter, n_components=n_components, perplexity=perplexity)
    X_embedded = tsne.fit_transform(x_s)

    # Plot
    axes[i].scatter(X_embedded[:, 0], X_embedded[:, 1], c=label_colors, label='t-SNE Embedding')
    axes[i].set_title(f'n_iter={n_iter}, n_components={n_components}, perplexity={perplexity}')
    axes[i].set_xticks([])
    axes[i].set_yticks([])
    axes[i].set_xlabel('Componente 1')
    axes[i].set_ylabel('Componente 2')
plt.tight_layout()
plt.show()

"""Al igual que con el Isomap, la elección del resultado de un modelo como mejor que otro es un proceso subjetivo.

Si nos basamos en los labels originales como por los que debería agrupar, podemos ver que los mejores resultados parecen ser los que tienen perplejidad 5, 500 iteraciones, y 2 componentes. En otras palabras, según las pruebas que hicimos, menor perplejidad y mayor cantidad de iteraciones parece producir modelos que transforman mejor el dataset en sólo 2 dimensiones.

Quizás, si quisiéramos elegir un modelo que contenga la mayor cantidad de varianza respecto al original, deberíamos elegir otro modelo.

#### Mejor resultado graficado en más detalle
"""

# Ajuste del modelo y transformación los datos
tsne = TSNE(n_iter=500, n_components=2, perplexity=5)
X_embedded = tsne.fit_transform(x_s)

# Plot
plt.figure(figsize=(10, 10))
plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=label_colors)
plt.legend()
custom_legend = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=label_color_mapping[label], label=label) for label in unique_labels]
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), handles=custom_legend, title='Tipo')
plt.title('Cultivos con Dimensiones Reducidas con t-SNE de 2 componentes, perplejidad 5, y 500 iteraciones')
plt.xlabel('Componente 1')
plt.ylabel('Componente 2')
plt.show()

"""### 6. K-Means

#### Análisis de K-means con distinto número de grupos
"""

Nc = range(1, clusters_max+1)
kmeans = [KMeans(n_clusters=i, n_init = 10) for i in Nc]
kmeans_fitted = [kmeans[i].fit(x_s) for i in range(clusters_max)]
scores = [kmeans_fitted[i].score(x_s) for i in range(clusters_max)]

plt.plot(Nc, scores)
plt.xlabel('Número de Clusters')
plt.ylabel('Puntuación de SSD')
plt.title('Curva del Codo de K-Means')
plt.show()

"""Podemos ver como al aumentar la cantidad de clusters, la puntuación de K-Means aumenta, ya que la sumatoria de las distancias entre los centroides de los clusters y los datos disminuye.

Si nos guiáramos por la regla del codo, nos quedaríamos con 7 u 8 clusters.

#### Cálculo de cantidad de grupos óptimo
"""

gs_obj = OptimalK(n_jobs=1, n_iter= 10, random_state = 42)
n_clusters = gs_obj(x_s, n_refs=50, cluster_array=np.arange(1, clusters_max+1))
n_clusters

"""El método GAP decidió el número máximo de clusters que le indicamos que podía elegir. Que es una cantidad de clusters mayor a la que nos hubiésemos quedado en caso de usar el método del codo.

#### Ajuste de modelo tipo K-means
"""

# Buscamos la asignación de clusters de el modelo ya entrenado de la cantidad de clusters que nos interesa
cluster_assignments = kmeans_fitted[n_clusters-1].predict(x_s) + 1

"""#### Gráfico en 3D"""

fig = px.scatter_3d(df, x='N', y='P', z='K', color=cluster_assignments, labels={'color': 'Cluster'})
fig.update_layout(title='K-Means Clustering de cultivos ploteado según 3 de sus componentes')
fig.show()

"""#### Silhouette Score"""

silhouette_score(x_s, cluster_assignments)

"""### 7. Clustering Jerárquico

#### Cálculo de cantidad de clústeres óptimos
"""

linkage_matrix = linkage(x_s, "ward")
dendrogram(linkage_matrix, no_labels = True)
plt.title("Dendrograma Inicial")
plt.show()

"""Podemos observar contando que luego de las agrupaciones de los datos en 10 clusters la diferencia entre las alturas de las divisiones en más clusters parece reducirse bastante, por lo tanto tomamos 10 como límite máximo de clusters con los que vamos a testear."""

clusters_max = 10

"""Método GAP"""

gs_obj = OptimalK(n_jobs=1, n_iter=40, random_state = 42)
n_clusters = gs_obj(x_s, n_refs=60, cluster_array=np.arange(2, clusters_max+1))
n_clusters

"""#### Ajuste de modelo de clustering aglomerativo"""

clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
cluster_assignments = clustering.fit_predict(x_s) + 1

"""#### Dendrograma final"""

linkage_matrix = linkage(x_s, "ward")
dendrogram(linkage_matrix, truncate_mode = 'lastp', p = n_clusters, show_leaf_counts = True)
plt.title("Dendrograma Final Truncado")
plt.show()

"""#### Gráfico en 3D pintado por clúster"""

fig = px.scatter_3d(df, x='N', y='P', z='K', color=cluster_assignments, labels={'color': 'Cluster'})
fig.update_layout(title='Clustering Aglomerativo de cultivos ploteado según 3 de sus componentes')
fig.show()

"""#### Silhouette score"""

silhouette_score(x_s, cluster_assignments)

"""### Conclusiones

#### Reducciones de dimensionalidad

Viendo los resultados de las reducciones de dimensionalidades de PCA, Isomap y t-SNE en 2 dimensiones, y juzgándolos por cuan bien distribuyen los distintos tipos de cultivos separados por grupos, el claro ganador es t-SNE, que puede ser un indicativo de que hay similitudes locales en el espacio de alta dimensionalidad de los datos.

Además, el hecho de que podamos ver bien los tipos de cultivos separados en grupos, sin haberle pasado dichos datos, significa que es una característica valiosa que puede aportar información sobre las demás.

Por otro lado, no hay una forma objetiva de juzgar este tipo de resultados, y si se tuviese otro objetivo con la reducción de dimensionalidad entonces el modelo más adecuado podría ser otro.

#### Agrupaciones

Comparando los Silhouette Score de ambos modelos que obtuvimos, K-Means (≈ 0.346) y Clustering Jerárquico (≈ 0.322), podemos ver que:
1. No encontró clusters que diesen puntuaciones altas, cosa que puede deberse a la naturaleza de los datos, o que deberíamos haber probado con más hiperparámetros.
2. No hay casi diferencia entre la performance de ambos modelos pero es preferible el K-Means (aunque es probable que se deba al mayor número de clusters que le dimos como parámetro).

Se podría haber reducido la cantidad de dimensiones de los datos con t-SNE antes de intentar agruparlos, pero viendo que tampoco eran muchas características y que había muchas instancias de entrenamiento, no creemos que los resultados se deban a la maldición de la dimensionalidad.
"""